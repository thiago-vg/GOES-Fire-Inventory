{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Emission Estimates from GOES-16 Data: FRP and Temperature\n",
    "\n",
    "This script is designed to process fire data retrieved from GOES-16 geostationary satellite observations. It specifically focuses on estimating fire-related emissions such as TPM (Total Particulate Matter), CO2, CO, and CH4 by utilizing the Fire Radiative Power (FRP) and temperature data. The script is structured to read the data, compute various emissions using the FEER (Fire Emission Estimation) coefficients, and save the results for specific grid regions in a CSV file. \n",
    "\n",
    "### Key Steps in the Script:\n",
    "\n",
    "1. **File Collection**: \n",
    "   - Collects a list of files within a specified time range (year, day, and hour).\n",
    "   - Each file contains data on fire radiative power (FRP), temperature, and other related variables.\n",
    "\n",
    "2. **Indexing**:\n",
    "   - Retrieves the relevant latitude and longitude coordinates for the region of interest.\n",
    "   - Extracts the indexes and matrix for the selected grid using geographic bounds (longitude and latitude).\n",
    "\n",
    "3. **Data Processing**:\n",
    "   - For each file, reads the FRP and temperature data.\n",
    "   - Uses the indexes matrix to extract the relevant data points within the defined grid area.\n",
    "   - Calculates emission estimates for Total Particulate Matter (TPM), CO2, CO, and CH4 using the FEER coefficients.\n",
    "   - Uncertainty values are calculated for the emission estimates.\n",
    "\n",
    "4. **Result Compilation**:\n",
    "   - Compiles results into a string format that includes the emission estimates and associated uncertainties.\n",
    "   - Stores the compiled results in a CSV file for further analysis.\n",
    "\n",
    "5. **Output**:\n",
    "   - A CSV file is generated with emissions data for each grid cell, containing detailed information on FRP, temperature, and the calculated emissions (TPM, CO2, CO, CH4) along with their uncertainties.\n",
    "\n",
    "This script enables the analysis of fire emissions over time and space, contributing to better understanding the impacts of biomass burning on air quality and climate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrdzr4LQo8Z4"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os  # Provides functions to interact with the operating system, such as file management\n",
    "from io import BytesIO  # Used for working with byte data in memory, often for handling file-like objects\n",
    "import s3fs  # Allows interaction with AWS S3 file systems\n",
    "import xarray as xr  # Provides advanced data structures and methods for working with multi-dimensional arrays\n",
    "import numpy as np  # Used for numerical operations, such as arrays, matrices, and mathematical functions\n",
    "import glob  # Used to retrieve files and directories matching a specified pattern\n",
    "from pyproj import Proj  # A Python interface for the PROJ library, used for coordinate transformations\n",
    "import pandas as pd  # Provides data structures and functions for data manipulation and analysis\n",
    "import warnings  # Allows for issuing warning messages in the code when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV8vxe-9q2z6"
   },
   "outputs": [],
   "source": [
    "# Define input and output directories and file names\n",
    "datadir = '/...'  # Directory where the processed CSV file will be saved\n",
    "\n",
    "# Collect the FEER (Fire Emission Inventory) emission coefficients CSV files\n",
    "data = sorted(glob.glob(datadir+'/FEER*.csv'))  # Retrieves all CSV files starting with 'FEER' in the data directory\n",
    "feer_data = pd.read_csv(data[0])  # Reads the first CSV file into a pandas DataFrame containing emission coefficients\n",
    "\n",
    "# Define output directory, the year to process the data, and the output file name\n",
    "# It is recommended to run this algorithm one year at a time\n",
    "Year = 2020  # The specific year of data to process\n",
    "outfile = datadir+'goes_data_emission_rate_test_'+str(Year)+'_150_350.csv'  # Output file name, including year and other details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeKrsO8QtNNf"
   },
   "outputs": [],
   "source": [
    "# Define constants for emission factors and species emission calculations\n",
    "\n",
    "# According to Nguyen and Wooster, 2020, the species emission can be calculated as:\n",
    "# Ce_species = (EF_species / EF_TMP) * Ce_TPM\n",
    "# where EF_species is the emission factor for the species and EF_TMP is the emission factor for TPM (Total Particulate Matter)\n",
    "\n",
    "# Emission factors (EF) for CO2, CO, CH4, and TPM in tropical forests (from Andreae, 2019)\n",
    "EF_CO2 = 1620  # Emission factor for CO2 in g/kg_burned\n",
    "s_EF_CO2 = 70  # Standard deviation of the CO2 emission factor\n",
    "\n",
    "EF_CO = 104  # Emission factor for CO in g/kg_burned\n",
    "s_EF_CO = 39  # Standard deviation of the CO emission factor\n",
    "\n",
    "EF_CH4 = 6.5  # Emission factor for CH4 in g/kg_burned\n",
    "s_EF_CH4 = 1.6  # Standard deviation of the CH4 emission factor\n",
    "\n",
    "EF_TPM = 8.7  # Emission factor for TPM in g/kg_burned\n",
    "s_EF_TPM = 3.1  # Standard deviation of the TPM emission factor\n",
    "\n",
    "# Calculate the emission coefficients (Ce) for CO2, CO, and CH4 in kg/MJ\n",
    "Ce_CO2 = (EF_CO2 / EF_TPM)  # Emission coefficient for CO2\n",
    "Ce_CO = (EF_CO / EF_TPM)    # Emission coefficient for CO\n",
    "Ce_CH4 = (EF_CH4 / EF_TPM)  # Emission coefficient for CH4\n",
    "\n",
    "# Calculate the uncertainty (sigma) in the emission coefficients using error propagation\n",
    "sigma_Ce_CO2 = np.sqrt((s_EF_CO2 / EF_TPM) ** 2 + (EF_CO2 * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "sigma_Ce_CO = np.sqrt((s_EF_CO / EF_TPM) ** 2 + (EF_CO * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "sigma_Ce_CH4 = np.sqrt((s_EF_CH4 / EF_TPM) ** 2 + (EF_CH4 * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "\n",
    "# Define the Region of Interest (ROI) in degrees (latitude and longitude)\n",
    "# The coordinates below correspond to the Amazon region\n",
    "minlon, maxlon, minlat, maxlat = -72, -48, -11, -3  # Coordinates for Amazon region ROI\n",
    "# For example, coordinates for the Cerrado region could be:\n",
    "# minlon, maxlon, minlat, maxlat = -57.5, -56.5, -17.5, -16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8jpjB-QtmC3"
   },
   "outputs": [],
   "source": [
    "# Define the file header for the output CSV\n",
    "aux1 = 'sat,year,julian,hhmm,code,central_lat,central_lon,FRP(MW),N_FRP,FRE(MJ),RE(kg/s),ME(kg),CO_2(kg),sigma_CO_2(kg),CO(kg),sigma_CO(kg),CH4(kg),sigma_CH4(kg),mean_FRP(MW),mean_temp(K)\\n'  \n",
    "# Define the header format for the output CSV file\n",
    "header = aux1  # The header consists of column names separated by commas\n",
    "outstring = ''  # Initialize an empty string to accumulate data (if needed)\n",
    "\n",
    "# Open the output file in write mode\n",
    "outfn = open(outfile, 'w')  \n",
    "outfn.writelines(header)  # Write the header to the output file\n",
    "\n",
    "# Initialize the S3 file system for interacting with AWS S3\n",
    "fs = s3fs.S3FileSystem(anon=True)  # Allows access to S3 buckets without authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv7n6K0QvdtA"
   },
   "outputs": [],
   "source": [
    "# Initialize geometric variables to extract satellite-specific data for the calculation of latitudes and longitudes\n",
    "\n",
    "# Function to extract latitude and longitude based on satellite coordinates\n",
    "def get_lat_lon(file_system):\n",
    "    # List files from the specified directory in the S3 bucket corresponding to a specific day and time\n",
    "    files = file_system.ls('noaa-goes16/ABI-L2-FDCF/2020/'+str(200).zfill(3)+'/'+str(15).zfill(2)+'/')  \n",
    "    # The list contains 6 files for day 200 of 2021 (for UTC times 15:00, 15:10, 15:20, ..., 15:50)\n",
    "\n",
    "    # Open the first file in the list to extract the dataset\n",
    "    with fs.open(files[0], 'rb') as f:\n",
    "        ds0 = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')  # Load the dataset into xarray\n",
    "\n",
    "    # Extract the satellite parameters from the dataset (projection height, origin longitude, and sweep axis)\n",
    "    sat_h = ds0.goes_imager_projection.perspective_point_height  # Satellite height (distance from Earth surface)\n",
    "    sat_lon = ds0.goes_imager_projection.longitude_of_projection_origin  # Longitude of the projection origin\n",
    "    sat_sweep = ds0.goes_imager_projection.sweep_angle_axis  # Sweep axis for satellite projection\n",
    "\n",
    "    # Use the PyProj library to set up the geostationary projection based on extracted satellite parameters\n",
    "    p = Proj(proj='geos', h=sat_h, lon_0=sat_lon, sweep=sat_sweep)  # Define the projection for the satellite view\n",
    "\n",
    "    # Calculate the X and Y coordinates in the satelliteâ€™s coordinate system (multiplying by satellite height)\n",
    "    X = np.array(ds0.x) * sat_h\n",
    "    Y = np.array(ds0.y) * sat_h\n",
    "\n",
    "    # Create a meshgrid for the X and Y coordinates to represent a grid of satellite points\n",
    "    XX, YY = np.meshgrid(X, Y)\n",
    "\n",
    "    # Transform the satellite coordinates (XX, YY) to latitudes and longitudes using the geostationary projection\n",
    "    rlon, rlat = p(XX, YY, inverse=True)\n",
    "\n",
    "    return rlat, rlon  # Return the calculated latitudes and longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYhiRnAMwVXs"
   },
   "outputs": [],
   "source": [
    "# Function to create a grid of 0.5Â° x 0.5Â° and assign the corresponding FEER coefficients to each grid element within the Region of Interest (ROI)\n",
    "def get_indexes_v3(min_lon, max_lon, min_lat, max_lat, rlat, rlon, dados_feer):\n",
    "    # Calculate the center of latitude and longitude for each element in a 0.5Â° x 0.5Â° grid inside the ROI\n",
    "    centers_lon = np.linspace(min_lon + 0.25, max_lon - 0.25, num=int(max_lon - min_lon) * 2)  # 0.5Â° grid\n",
    "    centers_lat = np.linspace(min_lat + 0.25, max_lat - 0.25, num=int(max_lat - min_lat) * 2)\n",
    "    \n",
    "    # Calculate the center of latitude and longitude for each element in a 1Â° x 1Â° grid (for comparison purposes)\n",
    "    centers_lon2 = np.linspace(min_lon + 0.5, max_lon - 0.5, num=int(max_lon - min_lon))  # 1Â° grid\n",
    "    centers_lat2 = np.linspace(min_lat + 0.5, max_lat - 0.5, num=int(max_lat - min_lat))\n",
    "\n",
    "    aux_list = []\n",
    "    \n",
    "    # Calculate the matching FEER coefficient for each element of the 1Â° x 1Â° grid\n",
    "    for i in range(0, len(centers_lat2)):\n",
    "        # Extract the FEER coefficients for the current latitude and longitude bounds from the 'dados_feer' DataFrame\n",
    "        df2 = dados_feer.loc[(dados_feer['Latitude'] == centers_lat2[i]) & \n",
    "                             (dados_feer['Longitude'] <= max_lon) &\n",
    "                             (dados_feer['Longitude'] >= min_lon), 'Ce_850'].to_numpy()\n",
    "        \n",
    "        # Append the corresponding FEER coefficients to the list\n",
    "        aux_list = np.append(aux_list, df2)\n",
    "        \n",
    "        if i == 0:\n",
    "            # Create a coordinate pair matrix for the 1Â° x 1Â° grid (latitudes and longitudes)\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            lat_lon_feer2 = np.column_stack((aux2, centers_lon2))\n",
    "        else:\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            aux2 = np.column_stack((aux2, centers_lon2))\n",
    "            lat_lon_feer2 = np.vstack((lat_lon_feer2, aux2))\n",
    "    \n",
    "    # Stack the matching FEER coefficients into the coordinate matrix for the 1Â° x 1Â° grid\n",
    "    lat_lon_feer2 = np.column_stack((lat_lon_feer2, aux_list))\n",
    "\n",
    "    # Calculate the FEER coefficient corresponding to each element of the 0.5Â° x 0.5Â° grid\n",
    "    for i in range(0, len(centers_lat)):\n",
    "        if i == 0:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            lat_lon_feer = np.column_stack((aux, centers_lon))\n",
    "        else:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            aux2 = np.column_stack((aux, centers_lon))\n",
    "            lat_lon_feer = np.vstack((lat_lon_feer, aux2))\n",
    "\n",
    "    # Match the corresponding FEER coefficient to each 0.5Â° x 0.5Â° grid element based on proximity\n",
    "    aux_list_2 = []\n",
    "    for j in range(0, len(lat_lon_feer)):\n",
    "        for n in range(0, len(lat_lon_feer2)):\n",
    "            if ((lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] + 0.25) or (lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] - 0.25)):\n",
    "                if ((lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] + 0.25) or (lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] - 0.25)):\n",
    "                    # Append the FEER coefficient to the auxiliary list for the matching grid\n",
    "                    aux_list_2 = np.append(aux_list_2, lat_lon_feer2[n, 2])\n",
    "    \n",
    "    # Stack the 0.5Â° x 0.5Â° grid coordinates and their corresponding FEER coefficients\n",
    "    lat_lon_feer = np.column_stack((lat_lon_feer, aux_list_2))\n",
    "    matrix = lat_lon_feer  # Final matrix of latitudes, longitudes, and FEER coefficients\n",
    "\n",
    "    # Create a mask for valid values within the entire ROI (used to select corresponding data from Full disk matrix)\n",
    "    I = np.where((rlat >= min_lat) & (rlat <= max_lat) & (rlon >= min_lon) & (rlon <= max_lon))\n",
    "    index_list = []\n",
    "    index_list.insert(0, I)\n",
    "\n",
    "    # Repeat the process for each element of the 0.5Â° x 0.5Â° grid to find corresponding indices in the satellite data\n",
    "    for k in range(0, len(matrix)):\n",
    "        aux1 = np.where((rlat >= matrix[k, 0] - 0.25) & (rlat <= matrix[k, 0] + 0.25) & \n",
    "                        (rlon >= matrix[k, 1] - 0.25) & (rlon <= matrix[k, 1] + 0.25))\n",
    "        index_list.insert(k + 1, aux1)\n",
    "\n",
    "    return index_list, matrix  # Return the list of indices and the final grid matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v225DN_TVnaT"
   },
   "outputs": [],
   "source": [
    "# Function to collect and save in a list the files that are in the informed period of interest\n",
    "def get_files(s_year, e_year, s_day, e_day, s_hour, e_hour):\n",
    "    print('Getting file names')\n",
    "    aux = []  # List to store the file paths\n",
    "    \n",
    "    # Iterate over the years from start to end year (inclusive)\n",
    "    for y in range(s_year, e_year + 1):\n",
    "        # Iterate over the days of the year\n",
    "        for d in range(s_day, e_day + 1):\n",
    "            # Iterate over the hours of the day\n",
    "            for j in range(s_hour, e_hour):\n",
    "                # Build the directory path based on year, day (zero-padded), and hour (zero-padded)\n",
    "                FD = fs.ls('noaa-goes16/ABI-L2-FDCF/' + str(y) + '/' + str(d).zfill(3) + '/' + str(j).zfill(2) + '/')\n",
    "                # Append the list of files found in that directory to the aux list\n",
    "                aux = np.append(aux, FD)\n",
    "\n",
    "    # Return the collected list of file paths\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ4Wr1OdxYOx"
   },
   "outputs": [],
   "source": [
    "#Main function. It will access the files and collect the products: FRP and Temperature\n",
    "#It will calculate the emission estimates using the FEER coefficients and the FRP\n",
    "#And save the spatial, temporal information and the results in a CSV file\n",
    "def process_data_v6(rlat,rlon,files,matrix,indexes):\n",
    "\n",
    "    #Open the directory to save the csv file\n",
    "    os.chdir(datadir)\n",
    "    #Loop through the files selected previously\n",
    "    for i in range(0,len(files)):\n",
    "      with fs.open(files[i], 'rb') as f:\n",
    "        #Open the files. The ds variable contain all the information inside the netCDF files\n",
    "        ds = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')\n",
    "        try:\n",
    "            #Gather the date-time information from the file\n",
    "            prodbase = files[i].split('/')[5][:23]\n",
    "            starttime=files[i].split(prodbase)[1].split('_')[0]\n",
    "            year,julian,hhmm=starttime[:4],starttime[4:7],starttime[7:11]\n",
    "            plottitle=year+','+julian+','+hhmm\n",
    "            fpart = starttime+','+plottitle\n",
    "            print('Processing year: {}, day: {}, hour: {}'.format(year,julian,hhmm)) #Processing Message\n",
    "            #the variable code is to convert the hours and minutes information in percentages of the day to facilitate future plots\n",
    "            code = int(julian)+(int(hhmm)/100)/24+(int(hhmm) % 100)/60/24\n",
    "\n",
    "            #####################################################################\n",
    "            P = np.array(ds.Power) #FRP Matrix data\n",
    "            T = np.array(ds.Temp)  #Temperature Matrix data\n",
    "            #A = np.array(ds.Area) #Burned area Matrix data\n",
    "\n",
    "            #Use the indexes matrix to collect the data only for the area of each element of the 0.5x0.5 grid\n",
    "            P_box_amazon = P[indexes[1]]\n",
    "            T_box_amazon = T[indexes[1]]\n",
    "\n",
    "            #The resulting elements using the index matrix does not have to main its matrix structure\n",
    "            #So the valid elements are transformed into an array to save processing demand\n",
    "            array_P_box_amazon = P_box_amazon[~np.isnan(P_box_amazon)]\n",
    "            array_T_box_amazon = T_box_amazon[~np.isnan(T_box_amazon)]\n",
    "\n",
    "\n",
    "            lat = matrix[0,0] #latitude of the element of the 0.5x0.5 grid\n",
    "            lon = matrix[0,1] #longitute of the element of the 0.5x0.5 grid\n",
    "            sum_frp = np.sum(array_P_box_amazon) #Total FRP on the grid element\n",
    "            N_frp = len(array_P_box_amazon) #Number of FRP data on the grid element\n",
    "            FRE = np.sum(array_P_box_amazon*600) #Total FRE\n",
    "            RE = np.sum(array_P_box_amazon*matrix[0,2]) #Multipling the FRP with the FEER emission coefficient results on the RATE of Emission of TPM\n",
    "            ME = np.sum(array_P_box_amazon*matrix[0,2]*600) #Multipling the FRE with the FEER emission coefficient results on the MASS of Emission of TPM\n",
    "            #And using the correlation with FEER coefficients and the tropical forest Factors of emission\n",
    "            #Calculate the estimates of emission of C02,CO and CH4\n",
    "            MCO2 = np.sum(array_P_box_amazon*Ce_CO2*matrix[0,2]*600)\n",
    "            MCO = np.sum(array_P_box_amazon*Ce_CO*matrix[0,2]*600)\n",
    "            MCH4 = np.sum(array_P_box_amazon*Ce_CH4*matrix[0,2]*600)\n",
    "\n",
    "            #Using the uncertanties of the Factors of emission calculate the uncertanties for the estimate of emission\n",
    "            s_RCO2 = array_P_box_amazon*sigma_Ce_CO2*matrix[0,2]\n",
    "            s_RCO = array_P_box_amazon*sigma_Ce_CO*matrix[0,2]\n",
    "            s_RCH4 = array_P_box_amazon*sigma_Ce_CH4*matrix[0,2]\n",
    "            s_MCO2 = (s_RCO2*600)**2\n",
    "            s_MCO = (s_RCO*600)**2\n",
    "            s_MCH4 = (s_RCH4*600)**2\n",
    "            s_sum_me_CO2 = np.sqrt(np.sum(s_MCO2))\n",
    "            s_sum_me_CO = np.sqrt(np.sum(s_MCO))\n",
    "            s_sum_me_CH4 = np.sqrt(np.sum(s_MCH4))\n",
    "\n",
    "            #Calculate the means of FRP and Temperature ignoring the warnings when there is NaN values\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                mean_temp = np.nanmean(array_T_box_amazon)\n",
    "                mean_frp = np.nanmean(array_P_box_amazon)\n",
    "            #Set the unvalid results as -9999\n",
    "            if np.isnan(mean_temp):\n",
    "                mean_temp = -9999\n",
    "            if np.isnan(mean_frp):\n",
    "                mean_frp = -9999\n",
    "\n",
    "\n",
    "            results = str(code) +','+ str(lat)+','+ str(lon)+','+ str(sum_frp)+','+ str(N_frp)+','\\\n",
    "                +str(FRE) +','+ str(RE)+','+ str(ME)+','+ str(MCO2)+','+ str(s_sum_me_CO2)+','\\\n",
    "                +str(MCO) +','+ str(s_sum_me_CO)+','+ str(MCH4)+','+ str(s_sum_me_CH4)+','+ str(mean_frp)+','+str(mean_temp)\n",
    "            #Compose results in a string and save them\n",
    "            outstring = fpart+','+results + '\\n'\n",
    "            outfn.writelines(outstring)\n",
    "\n",
    "            #Loop through all the 0.5x0.5 grid elements repeating the process\n",
    "            for k in range(1,len(matrix)):\n",
    "\n",
    "                P_box_amazon = P[indexes[k+1]]\n",
    "                array_P_box_amazon = P_box_amazon[~np.isnan(P_box_amazon)]\n",
    "                T_box_amazon = T[indexes[k+1]]\n",
    "                array_T_box_amazon = T_box_amazon[~np.isnan(T_box_amazon)]\n",
    "\n",
    "                lat = matrix[k,0]\n",
    "                lon = matrix[k,1]\n",
    "\n",
    "                sum_frp = np.sum(array_P_box_amazon)\n",
    "                N_frp = len(array_P_box_amazon)\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                    mean_temp = np.nanmean(array_T_box_amazon)\n",
    "                    mean_frp = np.nanmean(array_P_box_amazon)\n",
    "                if np.isnan(mean_temp):\n",
    "                    mean_temp = -9999\n",
    "                if np.isnan(mean_frp):\n",
    "                    mean_frp = -9999\n",
    "\n",
    "                FRE = np.sum(array_P_box_amazon*600)\n",
    "                RE = np.sum(array_P_box_amazon*matrix[k,2])\n",
    "                ME = np.sum(array_P_box_amazon*matrix[k,2]*600)\n",
    "                MCO2 = np.sum(array_P_box_amazon*Ce_CO2*matrix[k,2]*600)\n",
    "                MCO = np.sum(array_P_box_amazon*Ce_CO*matrix[k,2]*600)\n",
    "                MCH4 = np.sum(array_P_box_amazon*Ce_CH4*matrix[k,2]*600)\n",
    "\n",
    "\n",
    "                s_RCO2 = array_P_box_amazon*sigma_Ce_CO2*matrix[k,2]\n",
    "                s_RCO = array_P_box_amazon*sigma_Ce_CO*matrix[k,2]\n",
    "                s_RCH4 = array_P_box_amazon*sigma_Ce_CH4*matrix[k,2]\n",
    "                s_MCO2 = (s_RCO2*600)**2\n",
    "                s_MCO = (s_RCO*600)**2\n",
    "                s_MCH4 = (s_RCH4*600)**2\n",
    "                s_sum_me_CO2 = np.sqrt(np.sum(s_MCO2))\n",
    "                s_sum_me_CO = np.sqrt(np.sum(s_MCO))\n",
    "                s_sum_me_CH4 = np.sqrt(np.sum(s_MCH4))\n",
    "\n",
    "                # results = str(code)+','+ str(lat)+','+ str(lon)+','+ str(sum_frp)+','+ str(N_frp)\n",
    "                results = str(code) +','+ str(lat)+','+ str(lon)+','+ str(sum_frp)+','+ str(N_frp)+','\\\n",
    "                    +str(FRE) +','+ str(RE)+','+ str(ME)+','+ str(MCO2)+','+ str(s_sum_me_CO2)+','\\\n",
    "                    +str(MCO) +','+ str(s_sum_me_CO)+','+ str(MCH4)+','+ str(s_sum_me_CH4)+','+ str(mean_frp)+','+str(mean_temp)\n",
    "                #Compose results in a string and save them\n",
    "                outstring = fpart+','+results + '\\n'\n",
    "                outfn.writelines(outstring) #Write the results on the CSV file\n",
    "\n",
    "        #Catch a excepion and just warning\n",
    "        except OSError as error:\n",
    "             print(error)\n",
    "    #Close the file\n",
    "    outfn.close()\n",
    "    return print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6sZuoCvuCkr",
    "outputId": "e07e7cb0-a890-444a-9a05-f26197db1726"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#Starting message\n",
    "print('Compiling statistics')\n",
    "\n",
    "rlat,rlon = get_lat_lon(fs)\n",
    "\n",
    "Indexes,M = get_indexes_v3(minlon,maxlon,minlat,maxlat,rlat,rlon,feer_data)\n",
    "print('Got indexes and matrix')\n",
    "\n",
    "start_year,end_year,start_day,end_day,start_hour,end_our = Year,Year,150,151,15,16\n",
    "data_list = get_files(start_year,end_year,start_day,end_day,start_hour,end_our)\n",
    "print('Data listed')\n",
    "\n",
    "print('Starting process data')\n",
    "process_data_v6(rlat,rlon,data_list,M,Indexes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
