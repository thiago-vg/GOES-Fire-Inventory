{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get burned area data processing Code\n",
    "\n",
    "This script processes burned area data from NOAA GOES-16 for a given region of interest (ROI) in the Amazon, computes total and average burned area, and saves the results to a CSV file.\n",
    "\n",
    "1. Get Files Function\n",
    "The `get_files` function retrieves relevant satellite files within a specified year, day, and hour range. It handles missing files gracefully using a `try-except` block.\n",
    "\n",
    "2. Get Indexes and Matrix Function\n",
    "`get_indexes_v3` creates a 0.5째x0.5째 grid within the ROI and calculates the corresponding FEER coefficient for each grid cell, generating a matrix and a mask for valid data points.\n",
    "\n",
    "3. Process Data Area Function\n",
    "The `process_data_area` function processes each file by:\n",
    "- Extracting the date and time.\n",
    "- Calculating the total and average burned area for the ROI.\n",
    "- Handling missing data by setting NaNs to -9999.\n",
    "The results are saved in a CSV file with relevant details.\n",
    "\n",
    "4. Main Execution Flow\n",
    "- Latitude and longitude data are retrieved.\n",
    "- Grid indexes and FEER coefficients are computed.\n",
    "- A list of files is generated and processed.\n",
    "- The results are saved in a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrdzr4LQo8Z4"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# os: Provides functions to interact with the operating system, such as file and directory operations\n",
    "import os\n",
    "# BytesIO: Enables reading and writing of binary data in memory as if it were a file\n",
    "from io import BytesIO\n",
    "# s3fs: A Pythonic interface to Amazon S3, allowing for easy file operations on S3 buckets\n",
    "import s3fs\n",
    "# xarray: A powerful library for working with multi-dimensional arrays, particularly for geospatial and time-series data\n",
    "import xarray as xr\n",
    "# numpy: Provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions\n",
    "import numpy as np\n",
    "# glob: Used for finding all pathnames matching a specified pattern, useful for file pattern matching in directories\n",
    "import glob\n",
    "# pyproj: Provides tools for working with projections and coordinate transformations, such as converting between latitude/longitude and projected coordinates\n",
    "from pyproj import Proj\n",
    "# pandas: A data analysis library that provides data structures like DataFrame for handling structured data, useful for working with time-series and tabular data\n",
    "import pandas as pd\n",
    "# warnings: Used to issue warnings to the user, often to alert about potential issues or deprecated features\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djBYn8xQRVmi"
   },
   "outputs": [],
   "source": [
    "# Define input and output directories and files\n",
    "\n",
    "# datadir: Directory where the input data (CSV files with processed data) is located\n",
    "datadir = '/home/jovyan/Article_review/Data/'\n",
    "\n",
    "# glob: Search for all CSV files matching the pattern 'FEER*.csv' within the input directory\n",
    "data = sorted(glob.glob(datadir+'/FEER*.csv'))\n",
    "\n",
    "# Read the first matching CSV file (assumed to be the FEER data)\n",
    "feer_data = pd.read_csv(data[0])\n",
    "\n",
    "# Define output directory, year for data processing, and the output file name\n",
    "# It is recommended to process data one year at a time\n",
    "outdir = datadir  # Set the output directory to the same as the input directory\n",
    "Year = 2022  # Year for processing the data (modify as needed)\n",
    "outfile = outdir + 'goes_area_amazon_definitive_box_' + str(Year) + '_150_350.csv'  # Output file name based on the year\n",
    "\n",
    "# Define Region of Interest (ROI) in degrees (longitude and latitude)\n",
    "# The box represents the Amazon region by default\n",
    "minlon, maxlon, minlat, maxlat = -72, -48, -11, -3  # Amazon ROI\n",
    "# Alternative boxes (commented out examples)\n",
    "# minlon, maxlon, minlat, maxlat = -57.5, -56.5, -17.5, -16.5  # Cerrado large box\n",
    "# minlon, maxlon, minlat, maxlat = -72, -48, -9, -6  # Example Amazon box\n",
    "\n",
    "# Define the header for the output file\n",
    "aux1 = 'sat,year,julian,hhmm,code,sum_area,mean_area\\n'  # CSV header for output file\n",
    "header = aux1  # Set the header\n",
    "outstring = ''  # Placeholder for data that will be written\n",
    "outfn = open(outfile, 'w')  # Open output file in write mode\n",
    "outfn.writelines(header)  # Write the header to the output file\n",
    "outfn.close()  # Close the output file\n",
    "\n",
    "# Initialize S3 file system to access data stored on Amazon S3\n",
    "fs = s3fs.S3FileSystem(anon=True)  # Anonymous access to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmCglzWlRoxE"
   },
   "outputs": [],
   "source": [
    "# Initialize geometric variables\n",
    "\n",
    "# This function extracts the geometric values (latitude and longitude) from one file,\n",
    "# which contains satellite coordinates, and calculates the corresponding matrix of latitudes and longitudes.\n",
    "# These values will be used for geospatial referencing in the satellite data.\n",
    "\n",
    "def get_lat_lon(file_system):\n",
    "    # List of 6 files from the 'noaa-goes16' directory for the specified date (2022, day 200) and time (15:00 to 15:50 UTC)\n",
    "    # The directory structure includes data at 10-minute intervals\n",
    "    files = file_system.ls('noaa-goes16/ABI-L2-FDCF/2022/'+str(200).zfill(3)+'/'+str(15).zfill(2)+'/')\n",
    "\n",
    "    # Open the first file in the list to extract the geospatial information\n",
    "    # Here, we use 'h5netcdf' as the engine to read the data from the file\n",
    "    with fs.open(files[0], 'rb') as f:\n",
    "        ds0 = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')\n",
    "\n",
    "    # Extract satellite geometric parameters from the file\n",
    "    # These parameters define the satellite's position and the projection system used\n",
    "    sat_h = ds0.goes_imager_projection.perspective_point_height  # Satellite height\n",
    "    sat_lon = ds0.goes_imager_projection.longitude_of_projection_origin  # Longitude of the satellite's projection origin\n",
    "    sat_sweep = ds0.goes_imager_projection.sweep_angle_axis  # Sweep angle axis of the satellite\n",
    "\n",
    "    # Create a geostationary projection object using pyproj\n",
    "    # This projection is used to convert the satellite's (x, y) coordinates into geographic (latitude, longitude) coordinates\n",
    "    p = Proj(proj='geos', h=sat_h, lon_0=sat_lon, sweep=sat_sweep)\n",
    "\n",
    "    # Multiply the x and y coordinates by the satellite height to scale them properly for the projection\n",
    "    X = np.array(ds0.x) * sat_h\n",
    "    Y = np.array(ds0.y) * sat_h\n",
    "\n",
    "    # Create mesh grids for the x and y coordinates\n",
    "    XX, YY = np.meshgrid(X, Y)\n",
    "\n",
    "    # Convert the satellite projection coordinates (XX, YY) into latitude and longitude\n",
    "    rlon, rlat = p(XX, YY, inverse=True)\n",
    "\n",
    "    # Return the calculated latitude and longitude matrices\n",
    "    return rlat, rlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIG3XPxvR55Y"
   },
   "outputs": [],
   "source": [
    "# Function to collect and save file names in a list for the given period of interest with error handling\n",
    "# This function iterates over the specified year, day, and hour ranges and collects the corresponding file names\n",
    "# from the NOAA GOES-16 directory structure, with added error handling for missing files.\n",
    "\n",
    "def get_files(s_year, e_year, s_day, e_day, s_hour, e_hour):\n",
    "    print('Getting file names')\n",
    "    aux = []  # List to store the file names\n",
    "    # Loop over the years in the specified range\n",
    "    for y in range(s_year, e_year + 1):\n",
    "        # Loop over the days in the specified range\n",
    "        for d in range(s_day, e_day):\n",
    "            # The variable 'd' determines the days of the product (e.g., day 228 corresponds to 15:00, 15:10, etc.)\n",
    "            for j in range(s_hour, e_hour):\n",
    "                try:\n",
    "                    # List the files for a specific year, day, and hour directory\n",
    "                    # These directories contain 6 files for each 10-minute interval (e.g., 15:00, 15:10, ..., 15:50 UTC)\n",
    "                    FD = fs.ls('noaa-goes16/ABI-L2-FDCF/' + str(y) + '/' + str(d).zfill(3) + '/' + str(j).zfill(2) + '/')\n",
    "                    aux = np.append(aux, FD)  # Append the found files to the list\n",
    "                except FileNotFoundError as e:\n",
    "                    # In case a file is not found, print an error message and skip to the next file\n",
    "                    print(f\"FileNotFoundError file {'noaa-goes16/ABI-L2-FDCF/'+str(y)+'/'+str(d).zfill(3)+'/'+str(j).zfill(2)+'/'}: {e}. Skipping this file.\")\n",
    "                    continue  # Skip to the next file in the list\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFydSkkeUFnH"
   },
   "outputs": [],
   "source": [
    "# Function to create a 0.5째x0.5째 grid over a specified area and calculate the corresponding FEER coefficient for each grid element.\n",
    "# This function processes the latitude and longitude coordinates within a defined Region of Interest (ROI) and assigns\n",
    "# the FEER coefficients to a grid with two different grid resolutions (1째x1째 and 0.5째x0.5째). \n",
    "\n",
    "def get_indexes_v3(min_lon, max_lon, min_lat, max_lat, rlat, rlon, dados_feer):\n",
    "    # Calculate the center latitude and longitude of each grid element in the 0.5째x0.5째 grid within the ROI\n",
    "    centers_lon = np.linspace(min_lon + 0.25, max_lon - 0.25, num=int(max_lon - min_lon) * 2)\n",
    "    centers_lat = np.linspace(min_lat + 0.25, max_lat - 0.25, num=int(max_lat - min_lat) * 2)\n",
    "\n",
    "    # Calculate the center latitude and longitude of each grid element in the 1째x1째 grid within the ROI\n",
    "    centers_lon2 = np.linspace(min_lon + 0.5, max_lon - 0.5, num=int(max_lon - min_lon))\n",
    "    centers_lat2 = np.linspace(min_lat + 0.5, max_lat - 0.5, num=int(max_lat - min_lat))\n",
    "\n",
    "    aux_list = []  # List to store the FEER coefficients for the 1째x1째 grid\n",
    "    # Calculate the matching FEER coefficient for each element of the 1째x1째 grid\n",
    "    for i in range(len(centers_lat2)):\n",
    "        # Extract the corresponding FEER coefficient for the current 1째x1째 grid element from the 'dados_feer' DataFrame\n",
    "        df2 = dados_feer.loc[(dados_feer['Latitude'] == centers_lat2[i]) & \n",
    "                             (dados_feer['Longitude'] <= max_lon) & \n",
    "                             (dados_feer['Longitude'] >= min_lon), 'Ce_850'].to_numpy()\n",
    "        aux_list = np.append(aux_list, df2)\n",
    "        \n",
    "        # Create a grid of latitude and longitude coordinates for the 1째x1째 grid\n",
    "        if i == 0:\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            lat_lon_feer2 = np.column_stack((aux2, centers_lon2))\n",
    "        else:\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            aux2 = np.column_stack((aux2, centers_lon2))\n",
    "            lat_lon_feer2 = np.vstack((lat_lon_feer2, aux2))\n",
    "    \n",
    "    # Add the FEER coefficients to the grid of latitudes and longitudes for the 1째x1째 grid\n",
    "    lat_lon_feer2 = np.column_stack((lat_lon_feer2, aux_list))\n",
    "\n",
    "    # Calculate the FEER coefficient corresponding to each element of the 0.5째x0.5째 grid\n",
    "    for i in range(len(centers_lat)):\n",
    "        # Create a grid of latitude and longitude coordinates for the 0.5째x0.5째 grid\n",
    "        if i == 0:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            lat_lon_feer = np.column_stack((aux, centers_lon))\n",
    "        else:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            aux2 = np.column_stack((aux, centers_lon))\n",
    "            lat_lon_feer = np.vstack((lat_lon_feer, aux2))\n",
    "\n",
    "    # Create a list of latitudes, longitudes, and corresponding FEER coefficients for the 0.5째x0.5째 grid\n",
    "    aux_list_2 = []  # List to store the FEER coefficients for the 0.5째x0.5째 grid\n",
    "    for j in range(len(lat_lon_feer)):\n",
    "        for n in range(len(lat_lon_feer2)):\n",
    "            # Match the closest 1째x1째 grid element to the 0.5째x0.5째 grid element based on latitude and longitude\n",
    "            if (lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] + 0.25) or (lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] - 0.25):\n",
    "                if (lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] + 0.25) or (lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] - 0.25):\n",
    "                    aux_list_2 = np.append(aux_list_2, lat_lon_feer2[n, 2])\n",
    "    \n",
    "    # Add the matched FEER coefficients to the 0.5째x0.5째 grid\n",
    "    lat_lon_feer = np.column_stack((lat_lon_feer, aux_list_2))\n",
    "    matrix = lat_lon_feer  # Store the final matrix of latitudes, longitudes, and FEER coefficients\n",
    "\n",
    "    # Create a mask for valid data within the ROI\n",
    "    # This will be used to select the corresponding data from the full disk matrix based on latitude and longitude\n",
    "    I = np.where((rlat >= min_lat) & (rlat <= max_lat) & (rlon >= min_lon) & (rlon <= max_lon))\n",
    "    index_list = [I]  # Initialize the index list with the valid data points in the ROI\n",
    "\n",
    "    # Repeat the process for each element of the 0.5째x0.5째 grid\n",
    "    for k in range(len(matrix)):\n",
    "        # Find the matching grid elements for the 0.5째x0.5째 grid based on the latitude and longitude boundaries\n",
    "        aux1 = np.where((rlat >= matrix[k, 0] - 0.25) & (rlat <= matrix[k, 0] + 0.25) &\n",
    "                        (rlon >= matrix[k, 1] - 0.25) & (rlon <= matrix[k, 1] + 0.25))\n",
    "        index_list.append(aux1)  # Add the matching index to the list\n",
    "\n",
    "    return index_list, matrix  # Return the list of indices and the matrix with latitudes, longitudes, and FEER coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeV1VRqESCA7"
   },
   "outputs": [],
   "source": [
    "# Main function to process netCDF files, extract burned area data, and save results in a CSV file.\n",
    "# The function reads each file, retrieves spatial and temporal information, computes burned area statistics,\n",
    "# and stores the results in a CSV file for further analysis.\n",
    "def process_data_area(files, indexes):\n",
    "    # Change the current working directory to the output directory\n",
    "    os.chdir(outdir)\n",
    "    # Open the CSV file for writing the output data\n",
    "    outfn = open(outfile, 'w')\n",
    "    \n",
    "    # Loop through each file in the provided list\n",
    "    for i in range(0, len(files)):\n",
    "        with fs.open(files[i], 'rb') as f:\n",
    "            # Read the netCDF file and load the data into an xarray Dataset\n",
    "            ds = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')\n",
    "            try:\n",
    "                # Extract the date-time information from the file name\n",
    "                prodbase = files[i].split('/')[5][:23]  # Extract product base information\n",
    "                starttime = files[i].split(prodbase)[1].split('_')[0]  # Extract the start time\n",
    "                year, julian, hhmm = starttime[:4], starttime[4:7], starttime[7:11]  # Split date-time components\n",
    "                plottitle = year + ',' + julian + ',' + hhmm  # Create a plot title\n",
    "                fpart = starttime + ',' + plottitle  # Format the first part of the output string\n",
    "                print(f'Processing year: {year}, day: {julian}, hour: {hhmm}', end='\\r')  # Print progress message\n",
    "\n",
    "                # Calculate a fractional day from the start time\n",
    "                code = int(julian) + (int(hhmm) / 100) / 24 + (int(hhmm) % 100) / 60 / 24\n",
    "                \n",
    "                # Extract the burned area matrix from the dataset\n",
    "                A = np.array(ds.Area)\n",
    "\n",
    "                # Use the provided indexes to select the relevant region of interest (ROI)\n",
    "                A_box_amazon = A[indexes[0]]\n",
    "\n",
    "                # Flatten the valid (non-NaN) elements of the selected region into a 1D array\n",
    "                array_A_box_amazon = A_box_amazon[~np.isnan(A_box_amazon)]\n",
    "\n",
    "                # Calculate the total burned area by summing the valid values\n",
    "                sum_area = np.sum(array_A_box_amazon)\n",
    "\n",
    "                # Calculate the average burned area, ignoring warnings caused by NaN values\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                    mean_area = np.mean(array_A_box_amazon)\n",
    "\n",
    "                # Set the average to -9999 if it is NaN (invalid)\n",
    "                if np.isnan(mean_area):\n",
    "                    mean_area = -9999\n",
    "\n",
    "                # Format the results as a string\n",
    "                results = str(code) + ',' + str(sum_area) + ',' + str(mean_area)\n",
    "                # Combine the temporal information and the results into one string\n",
    "                outstring = fpart + ',' + results + '\\n'\n",
    "                # Write the results to the CSV file\n",
    "                outfn.writelines(outstring)\n",
    "\n",
    "            # Catch any OSError exceptions and print the error message\n",
    "            except OSError as error:\n",
    "                print(error)\n",
    "    \n",
    "    # Close the output file after processing all the data\n",
    "    outfn.close()\n",
    "    return print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "EJFFwr1dZ5No",
    "outputId": "13d09684-66db-43a4-9ba8-9a6215f5fce3"
   },
   "outputs": [],
   "source": [
    "# Retrieve latitude and longitude data using the get_lat_lon function\n",
    "rlat, rlon = get_lat_lon(fs)\n",
    "\n",
    "# Get the indexes and matrix corresponding to the region of interest (ROI) from the FEER data\n",
    "Indexes, M = get_indexes_v3(minlon, maxlon, minlat, maxlat, rlat, rlon, feer_data)\n",
    "\n",
    "print('Got indexes and matrix')  # Print a confirmation message once the index and matrix are obtained\n",
    "\n",
    "# Define the year, day, and hour range for the data extraction\n",
    "start_year, end_year = Year, Year  # Set both start and end year to the specified Year\n",
    "start_day, end_day = 150, 350  # Define the start and end days of the year\n",
    "start_hour, end_hour = 0, 24  # Define the start and end hours for data extraction\n",
    "\n",
    "# Get the list of files based on the specified time range\n",
    "data_list = get_files(start_year, end_year, start_day, end_day, start_hour, end_hour)\n",
    "\n",
    "print('Data listed')  # Print a confirmation message once the data files are listed\n",
    "\n",
    "print('Starting process data')  # Indicate that the data processing has started\n",
    "# Process the data files for the ROI using the previously obtained indexes\n",
    "process_data_area(data_list, Indexes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
