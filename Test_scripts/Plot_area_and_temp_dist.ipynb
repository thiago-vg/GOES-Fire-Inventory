{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bimodal Gaussian Fit for Temperature Data\n",
    "\n",
    "This code performs the following steps to analyze temperature data and fit a bimodal Gaussian distribution to it:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - The temperature data is extracted from the dataset and filtered to exclude values below 400K.\n",
    "   - The logarithm of the temperature values is applied for further analysis.\n",
    "\n",
    "2. **Data Segmentation**:\n",
    "   - The temperature data is separated into two groups: below and above a threshold (6.68), representing the smoldering and flaming phases of the fire, respectively.\n",
    "\n",
    "3. **Histogram Calculation**:\n",
    "   - A histogram is created for the data with relative frequency, using weights to account for the distribution of values.\n",
    "   - The histogram is used to analyze the distribution of the logarithmic temperature values.\n",
    "\n",
    "4. **Bimodal Gaussian Fit**:\n",
    "   - The histogram data is fitted using a bimodal Gaussian function. Initial estimates for the Gaussian parameters are provided based on the data.\n",
    "   - The parameters for the two modes (smoldering and flaming) are optimized using `curve_fit`.\n",
    "\n",
    "5. **Plotting and Visualization**:\n",
    "   - A histogram is plotted along with the fitted bimodal Gaussian curve and individual Gaussian components for the smoldering and flaming phases.\n",
    "   - The exponentiated means and their bounds are also calculated to show the actual temperature values for both phases.\n",
    "   - The plot is customized with titles, labels, and a legend to provide a clear and informative visualization of the temperature distribution.\n",
    "\n",
    "The goal of this analysis is to model the temperature distribution using a bimodal Gaussian, which helps distinguish between different phases of fire activity and understand the temperature dynamics of biomass burning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0ONiLUZIXUB"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, plotting, statistical analysis, and signal processing\n",
    "\n",
    "# pandas for handling data in DataFrame format\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib for creating static, animated, and interactive plots\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# glob for file path manipulation and finding files that match a given pattern\n",
    "import glob\n",
    "\n",
    "# numpy for numerical operations and array handling\n",
    "import numpy as np\n",
    "\n",
    "# math module for basic mathematical functions\n",
    "import math as mt\n",
    "\n",
    "# scipy: various submodules for advanced scientific computations\n",
    "from scipy import stats  # for statistical functions and distributions\n",
    "from scipy.optimize import curve_fit  # for curve fitting optimization\n",
    "from scipy.stats import lognorm, norm  # for lognormal and normal distributions\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable  # for creating grid layouts in plots\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable  # for fine-tuning axes\n",
    "from scipy.signal import chirp, find_peaks, peak_widths, find_peaks_cwt  # for signal analysis and peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v991YrVqaK_w"
   },
   "outputs": [],
   "source": [
    "# Define the directory where the data files are located\n",
    "datadir = '.../Data'\n",
    "\n",
    "# Define the year for processing as a string (e.g., 2020)\n",
    "Year = str(2020)\n",
    "\n",
    "# Example file name formats: 'array_T_data_2020_150_350.csv' for temperature data and 'goes_area_amazon_definitive_box_2020_150_350.csv' for area data\n",
    "# Collect all the relevant files based on the year and specific naming pattern\n",
    "data_area = sorted(glob.glob(datadir+'/*goes_area_amazon_definitive_box_'+str(Year)+'_150_350.csv'))\n",
    "data_temp = sorted(glob.glob(datadir+'/*array_T_data_'+Year+'_150_350.csv'))\n",
    "\n",
    "# Read the CSV file containing area data for the specified year and region\n",
    "dados_area = pd.read_csv(data_area[0])\n",
    "\n",
    "# Read the CSV file containing temperature data for the specified year and region\n",
    "temperature_data = pd.read_csv(data_temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a graph of the distribution of the total burned area\n",
    "# Extract the header from the data file to list all columns\n",
    "header = list(dados_area)\n",
    "\n",
    "# Extract the 'sum_area' column from the data and convert it to a numpy array, dropping any NaN values\n",
    "pos3 = header.index('sum_area')\n",
    "column3 = dados_area.iloc[:, pos3]\n",
    "area = np.array(column3.dropna(), dtype=float)\n",
    "\n",
    "# Configure the plot's appearance\n",
    "plt.clf()  # Clear any previous figures\n",
    "plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "plt.xlabel('ln(Total active fire burning area / Ao)', fontsize=20)  # Label for the x-axis\n",
    "plt.ylabel('Relative frequency', fontsize=20)  # Label for the y-axis\n",
    "plt.tick_params(axis='x', labelsize=18)  # Set font size for x-axis ticks\n",
    "plt.tick_params(axis='y', labelsize=18)  # Set font size for y-axis ticks\n",
    "\n",
    "# Select valid (positive) values for the area and apply the natural logarithm transformation\n",
    "index = np.where((area > 0))  # Find indices where the area is greater than 0\n",
    "area_index = area[index]  # Get the area values at those indices\n",
    "area_index = np.log(area_index)  # Take the natural logarithm of the area values\n",
    "\n",
    "# Define the limits and bin edges for the histogram plot\n",
    "limites = [8, 20]  # The range of x-values to display in the histogram\n",
    "pontos_divisao = np.arange(8, 20, 0.2)  # The bin edges for the histogram\n",
    "\n",
    "# Calculate the relative frequencies for the histogram (normalized)\n",
    "frequencias_relativas = area_index / len(area_index)\n",
    "\n",
    "# Plot the histogram of the logarithmic area values\n",
    "plt.title(str(Year), fontsize=20)  # Set the plot title to the year\n",
    "Mean = 'Mean: {:.2f}\\nMedian: {:.2f}\\nN: {:.0f}'.format(np.mean(area_index), np.median(area_index), len(area_index))  # Prepare the mean, median, and count for the legend\n",
    "plt.hist(area_index, range=limites, bins=pontos_divisao, weights=frequencias_relativas, label=Mean, color='gray')  # Plot the histogram with relative frequencies\n",
    "plt.axvline(np.percentile(area_index, 10), color='0.0', label='percentile 10')  # Mark the 10th percentile with a vertical line\n",
    "plt.axvline(np.percentile(area_index, 90), color='0.3', linestyle='--', label='percentile 90')  # Mark the 90th percentile with a dashed vertical line\n",
    "plt.legend(loc='best', fontsize=16)  # Display the legend in the best location with appropriate font size\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "# plt.savefig('filename.png', dpi=200)  # Uncomment to save the figure if needed\n",
    "\n",
    "# Print out the mean, median, and percentiles of the area (back-transformed to the original scale)\n",
    "mean_value = np.exp(np.mean(area_index)) / 1e6  # Convert the mean from log scale to original scale (in millions)\n",
    "median_value = np.exp(np.median(area_index)) / 1e6  # Convert the median from log scale to original scale (in millions)\n",
    "n_value = len(area_index)  # The number of valid area entries\n",
    "percentile_10 = np.exp(np.percentile(area_index, 10)) / 1e6  # Convert the 10th percentile from log scale to original scale (in millions)\n",
    "percentile_90 = np.exp(np.percentile(area_index, 90)) / 1e6  # Convert the 90th percentile from log scale to original scale (in millions)\n",
    "\n",
    "# Print the summary statistics to the console\n",
    "print(f\"Mean: {mean_value:.3f}\")\n",
    "print(f\"Median: {median_value:.3f}\")\n",
    "print(f\"N: {n_value}\")\n",
    "print(f\"10th Percentile: {percentile_10:.4f}\")\n",
    "print(f\"90th Percentile: {percentile_90:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bimodal Gaussian function\n",
    "# This function models a mixture of two Gaussian (normal) distributions.\n",
    "# It takes the following parameters:\n",
    "# - x: The input data for the distribution.\n",
    "# - amplitude1: The amplitude (height) of the first Gaussian curve.\n",
    "# - mean1: The mean (center) of the first Gaussian curve.\n",
    "# - std1: The standard deviation (spread) of the first Gaussian curve.\n",
    "# - amplitude2: The amplitude (height) of the second Gaussian curve.\n",
    "# - mean2: The mean (center) of the second Gaussian curve (fixed based on literature).\n",
    "# - std2: The standard deviation (spread) of the second Gaussian curve (fixed based on literature).\n",
    "\n",
    "def gaussian_bimodal(x, amplitude1, mean1, std1, amplitude2, mean2, std2):\n",
    "    # Lock the second Gaussian's mean and standard deviation based on literature values\n",
    "    mean2 = 6.90  # Fixed mean for the second Gaussian curve\n",
    "    std2 = 0.18  # Fixed standard deviation for the second Gaussian curve\n",
    "    \n",
    "    # Compute and return the sum of two Gaussian functions\n",
    "    return (amplitude1 * np.exp(-(x - mean1) ** 2 / (2 * std1 ** 2)) +\n",
    "            amplitude2 * np.exp(-(x - mean2) ** 2 / (2 * std2 ** 2)))\n",
    "\n",
    "# Gaussian function\n",
    "# This function models a single Gaussian (normal) distribution.\n",
    "# It takes the following parameters:\n",
    "# - x: The input data for the distribution.\n",
    "# - amplitude1: The amplitude (height) of the Gaussian curve.\n",
    "# - mean1: The mean (center) of the Gaussian curve.\n",
    "# - std1: The standard deviation (spread) of the Gaussian curve.\n",
    "\n",
    "def gaussian(x, amplitude1, mean1, std1):\n",
    "    # Compute and return the value of a single Gaussian function\n",
    "    return amplitude1 * np.exp(-(x - mean1) ** 2 / (2 * std1 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDHV-4fsU3oU"
   },
   "outputs": [],
   "source": [
    "# List the data file header\n",
    "header = list(temperature_data)\n",
    "\n",
    "# Extract the 'array_temp' column from the file\n",
    "pos1 = header.index('array_temp')  # Find the position of the 'array_temp' column\n",
    "column1 = temperature_data.iloc[:, pos1]  # Select the column data\n",
    "Temp = np.array(column1.dropna(), dtype=float)  # Convert to numpy array and drop missing values\n",
    "\n",
    "# Exclude the data below the 400K threshold and apply the logarithm to the temperature data\n",
    "Temp = Temp[Temp > 400]  # Filter out values below 400K\n",
    "Temp = np.log(Temp)  # Apply the logarithm transformation to the remaining data\n",
    "\n",
    "# Separate the data into two groups based on the flaming threshold (~800K)\n",
    "data1 = Temp[Temp <= 6.68]  # Data below the flaming threshold\n",
    "data2 = Temp[Temp > 6.68]  # Data above the flaming threshold\n",
    "data = np.concatenate([data1, data2])  # Combine both groups into one dataset\n",
    "\n",
    "# Calculate the relative frequency (since np.histogram with weights doesn't work for log-transformed data)\n",
    "relative_freq = data / len(data)\n",
    "\n",
    "# Create a histogram with relative frequency\n",
    "num_bins = 200  # Define the number of bins for the histogram\n",
    "hist, bin_edges = np.histogram(data, bins=num_bins, range=(min(data), max(data)), weights=relative_freq)  # Generate histogram data\n",
    "\n",
    "# Calculate the bin centers for plotting\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Compute the midpoint of each bin\n",
    "\n",
    "# Define initial parameters for the bimodal fit based on the data\n",
    "amplitude2_initial = np.max(data2)  # Initial amplitude for the second Gaussian\n",
    "mean2_initial = 6.90  # Initial mean for the second Gaussian (based on literature)\n",
    "std2_initial = 0.18  # Initial standard deviation for the second Gaussian (based on literature)\n",
    "\n",
    "# Set up initial parameter estimates for the curve fitting function\n",
    "parameters_initial = [np.max(hist), np.mean(data1), np.std(data1),\n",
    "                      amplitude2_initial, mean2_initial, std2_initial]\n",
    "\n",
    "# Perform curve fitting using the initial parameters\n",
    "parameters_optimized, _ = curve_fit(gaussian_bimodal, bin_centers, hist, p0=parameters_initial)\n",
    "\n",
    "# Extract the optimized parameters from the fitting result\n",
    "amplitude1_optimized, mean1_optimized, std1_optimized, amplitude2_optimized, mean2_optimized, std2_optimized = parameters_optimized\n",
    "\n",
    "# Set up the figure for plotting\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define the x-values for plotting the fitted curve\n",
    "x_curve = np.linspace(min(data), max(data), 1000)  # Create a smooth range of values for plotting the fit\n",
    "\n",
    "# Calculate the y-values for the bimodal fit\n",
    "y_curve = gaussian_bimodal(x_curve, amplitude1_optimized, mean1_optimized, std1_optimized,\n",
    "                            amplitude2_optimized, mean2_optimized, std2_optimized)\n",
    "\n",
    "# Format the plot label with statistics\n",
    "Label = 'N_total = {:.0f}\\nσ1={:.2f},σ2={:.2f}\\nMode_1={:.2f},Mode_2={:.2f}'.format(len(Temp), std1_optimized, std2_optimized, mean1_optimized, mean2_optimized)\n",
    "\n",
    "# Plot the histogram with the fitted curve\n",
    "plt.hist(data, bins=num_bins, range=(min(data), max(data)), weights=relative_freq, alpha=0.6, label=Label, color='gray')\n",
    "plt.plot(x_curve, y_curve, label='Bimodal fit', color='0.0')\n",
    "\n",
    "# Plot individual Gaussian fits for each phase (smoldering and flaming)\n",
    "x_curve_individual = np.linspace(min(data), max(data), 1000)\n",
    "y_curve_individual_1 = gaussian(x_curve_individual, amplitude1_optimized, mean1_optimized, std1_optimized)\n",
    "y_curve_individual_2 = gaussian(x_curve_individual, amplitude2_optimized, mean2_optimized, std2_optimized)\n",
    "\n",
    "plt.plot(x_curve_individual, y_curve_individual_1, '--', label='Smoldering phase fit', color='0.5')\n",
    "plt.plot(x_curve_individual, y_curve_individual_2, '-.', label='Flaming phase fit', color='0.2')\n",
    "\n",
    "# Customize x-axis tick marks\n",
    "plt.xticks(np.arange(6, 8.5, 0.5))\n",
    "\n",
    "# Add plot titles and labels\n",
    "plt.title(Year)\n",
    "plt.xlabel('ln(T/T0)')\n",
    "plt.ylabel('Relative frequency')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot (uncomment to save as an image file)\n",
    "# plt.savefig('filename.png', dpi=300)\n",
    "\n",
    "# Exponentiate the means to convert them back to the original temperature scale\n",
    "mean1_optimized_exp = np.exp(mean1_optimized)  # Exponentiate mean1\n",
    "mean2_optimized_exp = np.exp(mean2_optimized)  # Exponentiate mean2\n",
    "\n",
    "# Calculate the bounds for the means based on 1 standard deviation\n",
    "mean1_lower = mean1_optimized - std1_optimized\n",
    "mean1_upper = mean1_optimized + std1_optimized\n",
    "mean2_lower = mean2_optimized - std2_optimized\n",
    "mean2_upper = mean2_optimized + std2_optimized\n",
    "\n",
    "# Exponentiate the bounds to convert them back to the original temperature scale\n",
    "mean1_lower_exp = np.abs(mean1_optimized_exp - np.exp(mean1_lower))  # Lower bound for mean1\n",
    "mean1_upper_exp = np.abs(mean1_optimized_exp - np.exp(mean1_upper))  # Upper bound for mean1\n",
    "mean2_lower_exp = np.abs(mean2_optimized_exp - np.exp(mean2_lower))  # Lower bound for mean2\n",
    "mean2_upper_exp = np.abs(mean2_optimized_exp - np.exp(mean2_upper))  # Upper bound for mean2\n",
    "\n",
    "# Format the label to include the exponentiated means and their bounds\n",
    "Label = ('N_total = {:.0f}\\n'\n",
    "         'σ1={:.2f} (Lower: {:.2f}, Upper: {:.2f}), '\n",
    "         'σ2={:.2f} (Lower: {:.2f}, Upper: {:.2f})\\n'\n",
    "         'Mode_1={:.2f}, Mode_2={:.2f}'.format(\n",
    "    len(Temp), \n",
    "    std1_optimized, mean1_lower_exp, mean1_upper_exp,  # Show mean1 with std bounds\n",
    "    std2_optimized, mean2_lower_exp, mean2_upper_exp,  # Show mean2 with std bounds\n",
    "    mean1_optimized_exp, mean2_optimized_exp  # Actual means\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
