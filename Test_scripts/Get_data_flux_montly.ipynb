{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAlBjUIBBv0m"
   },
   "source": [
    "# Flux of Emission Estimates from GOES-16 Data: FRP and Burned Area\n",
    "\n",
    "This script is designed to process fire data retrieved from GOES-16 geostationary satellite observations. It focuses on estimating fire-related emissions such as TPM (Total Particulate Matter), CO2, CO, and CH4 by utilizing the Fire Radiative Power (FRP) and burned area data. The script is structured to read the data, calculate the flux of emission for each grid cell using the FEER (Fire Emission Estimation) coefficients, and save the results for specific grid regions in a CSV file. \n",
    "\n",
    "### Key Steps in the Script:\n",
    "\n",
    "1. **File Collection**: \n",
    "   - Collects a list of files within a specified time range (year, day, and hour).\n",
    "   - Each file contains data on fire radiative power (FRP), burned area, and other related variables.\n",
    "\n",
    "2. **Indexing**:\n",
    "   - Retrieves the relevant latitude and longitude coordinates for the region of interest.\n",
    "   - Extracts the indexes and matrix for the selected grid using geographic bounds (longitude and latitude).\n",
    "\n",
    "3. **Data Processing**:\n",
    "   - For each file, reads the FRP and burned area data.\n",
    "   - Uses the indexes matrix to extract the relevant data points within the defined grid area.\n",
    "   - Calculates the emission flux for each grid cell based on FRP and burned area, using the FEER coefficients to estimate the Total Particulate Matter (TPM), CO2, CO, and CH4 flux of emissions.\n",
    "   - The emissions are averaged over the specified time period (e.g., monthly), providing estimates for each grid cell.\n",
    "\n",
    "4. **Result Compilation**:\n",
    "   - For each grid cell, the script computes the monthly average emission flux values for TPM, CO2, CO, and CH4.\n",
    "   - The results, including the calculated emission fluxes for each grid cell and the associated latitudes and longitudes, are compiled into a string format.\n",
    "   - These results are saved in a CSV file for further analysis.\n",
    "\n",
    "5. **Output**:\n",
    "   - A CSV file is generated containing emissions data for each grid cell. This file includes detailed information on latitude,longitude, and the calculated emissions (TPM, CO2, CO, CH4) for each cell, allowing for spatial and temporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrdzr4LQo8Z4"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from io import BytesIO\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "from pyproj import Proj\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV8vxe-9q2z6"
   },
   "outputs": [],
   "source": [
    "# Define input and output directories and file names\n",
    "datadir = '/...'  # Directory where the processed CSV file will be saved\n",
    "\n",
    "# Collect the FEER (Fire Emission Inventory) emission coefficients CSV files\n",
    "data = sorted(glob.glob(datadir+'/FEER*.csv'))  # Retrieves all CSV files starting with 'FEER' in the data directory\n",
    "feer_data = pd.read_csv(data[0])  # Reads the first CSV file into a pandas DataFrame containing emission coefficients\n",
    "\n",
    "# Define output directory, the year to process the data, and the output file name\n",
    "# It is recommended to run this algorithm one year at a time\n",
    "Year = 2020  # The specific year of data to process\n",
    "outfile = datadir+'amazon_'+str(Year)+'_mean_montly_flux.csv' # Output file name, including year and other details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeKrsO8QtNNf"
   },
   "outputs": [],
   "source": [
    "# Define constants for emission factors and species emission calculations\n",
    "\n",
    "# According to Nguyen and Wooster, 2020, the species emission can be calculated as:\n",
    "# Ce_species = (EF_species / EF_TMP) * Ce_TPM\n",
    "# where EF_species is the emission factor for the species and EF_TMP is the emission factor for TPM (Total Particulate Matter)\n",
    "\n",
    "# Emission factors (EF) for CO2, CO, CH4, and TPM in tropical forests (from Andreae, 2019)\n",
    "EF_CO2 = 1620  # Emission factor for CO2 in g/kg_burned\n",
    "s_EF_CO2 = 70  # Standard deviation of the CO2 emission factor\n",
    "\n",
    "EF_CO = 104  # Emission factor for CO in g/kg_burned\n",
    "s_EF_CO = 39  # Standard deviation of the CO emission factor\n",
    "\n",
    "EF_CH4 = 6.5  # Emission factor for CH4 in g/kg_burned\n",
    "s_EF_CH4 = 1.6  # Standard deviation of the CH4 emission factor\n",
    "\n",
    "EF_TPM = 8.7  # Emission factor for TPM in g/kg_burned\n",
    "s_EF_TPM = 3.1  # Standard deviation of the TPM emission factor\n",
    "\n",
    "# Calculate the emission coefficients (Ce) for CO2, CO, and CH4 in kg/MJ\n",
    "Ce_CO2 = (EF_CO2 / EF_TPM)  # Emission coefficient for CO2\n",
    "Ce_CO = (EF_CO / EF_TPM)    # Emission coefficient for CO\n",
    "Ce_CH4 = (EF_CH4 / EF_TPM)  # Emission coefficient for CH4\n",
    "\n",
    "# Calculate the uncertainty (sigma) in the emission coefficients using error propagation\n",
    "sigma_Ce_CO2 = np.sqrt((s_EF_CO2 / EF_TPM) ** 2 + (EF_CO2 * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "sigma_Ce_CO = np.sqrt((s_EF_CO / EF_TPM) ** 2 + (EF_CO * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "sigma_Ce_CH4 = np.sqrt((s_EF_CH4 / EF_TPM) ** 2 + (EF_CH4 * s_EF_TPM / (EF_TPM) ** 2) ** 2)\n",
    "\n",
    "# Define the Region of Interest (ROI) in degrees (latitude and longitude)\n",
    "# The coordinates below correspond to the Amazon region\n",
    "minlon, maxlon, minlat, maxlat = -72, -48, -11, -3  # Coordinates for Amazon region ROI\n",
    "# For example, coordinates for the Cerrado region could be:\n",
    "# minlon, maxlon, minlat, maxlat = -57.5, -56.5, -17.5, -16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8jpjB-QtmC3"
   },
   "outputs": [],
   "source": [
    "#Define the file header\n",
    "aux1 ='year,month,central_lat,central_lon,mean_flux,TPM_flux,CO2_flux,CO_flux,CH4_flux\\n'\n",
    "header = aux1\n",
    "outstring=''\n",
    "outfn = open(outfile, 'w')\n",
    "outfn.writelines(header)\n",
    "\n",
    "# Define the file header for the output CSV\n",
    "aux1 ='year,month,central_lat,central_lon,mean_flux,TPM_flux,CO2_flux,CO_flux,CH4_flux\\n'\n",
    "# Define the header format for the output CSV file\n",
    "header = aux1  # The header consists of column names separated by commas\n",
    "outstring = ''  # Initialize an empty string to accumulate data (if needed)\n",
    "\n",
    "# Open the output file in write mode\n",
    "outfn = open(outfile, 'w')  \n",
    "outfn.writelines(header)  # Write the header to the output file\n",
    "\n",
    "# Initialize the S3 file system for interacting with AWS S3\n",
    "fs = s3fs.S3FileSystem(anon=True)  # Allows access to S3 buckets without authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv7n6K0QvdtA"
   },
   "outputs": [],
   "source": [
    "# Initialize geometric variables to extract satellite-specific data for the calculation of latitudes and longitudes\n",
    "\n",
    "# Function to extract latitude and longitude based on satellite coordinates\n",
    "def get_lat_lon(file_system):\n",
    "    # List files from the specified directory in the S3 bucket corresponding to a specific day and time\n",
    "    files = file_system.ls('noaa-goes16/ABI-L2-FDCF/2020/'+str(200).zfill(3)+'/'+str(15).zfill(2)+'/')  \n",
    "    # The list contains 6 files for day 200 of 2021 (for UTC times 15:00, 15:10, 15:20, ..., 15:50)\n",
    "\n",
    "    # Open the first file in the list to extract the dataset\n",
    "    with fs.open(files[0], 'rb') as f:\n",
    "        ds0 = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')  # Load the dataset into xarray\n",
    "\n",
    "    # Extract the satellite parameters from the dataset (projection height, origin longitude, and sweep axis)\n",
    "    sat_h = ds0.goes_imager_projection.perspective_point_height  # Satellite height (distance from Earth surface)\n",
    "    sat_lon = ds0.goes_imager_projection.longitude_of_projection_origin  # Longitude of the projection origin\n",
    "    sat_sweep = ds0.goes_imager_projection.sweep_angle_axis  # Sweep axis for satellite projection\n",
    "\n",
    "    # Use the PyProj library to set up the geostationary projection based on extracted satellite parameters\n",
    "    p = Proj(proj='geos', h=sat_h, lon_0=sat_lon, sweep=sat_sweep)  # Define the projection for the satellite view\n",
    "\n",
    "    # Calculate the X and Y coordinates in the satellite’s coordinate system (multiplying by satellite height)\n",
    "    X = np.array(ds0.x) * sat_h\n",
    "    Y = np.array(ds0.y) * sat_h\n",
    "\n",
    "    # Create a meshgrid for the X and Y coordinates to represent a grid of satellite points\n",
    "    XX, YY = np.meshgrid(X, Y)\n",
    "\n",
    "    # Transform the satellite coordinates (XX, YY) to latitudes and longitudes using the geostationary projection\n",
    "    rlon, rlat = p(XX, YY, inverse=True)\n",
    "\n",
    "    return rlat, rlon  # Return the calculated latitudes and longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYhiRnAMwVXs"
   },
   "outputs": [],
   "source": [
    "# Function to create a grid of 0.5° x 0.5° and assign the corresponding FEER coefficients to each grid element within the Region of Interest (ROI)\n",
    "def get_indexes_v3(min_lon, max_lon, min_lat, max_lat, rlat, rlon, dados_feer):\n",
    "    # Calculate the center of latitude and longitude for each element in a 0.5° x 0.5° grid inside the ROI\n",
    "    centers_lon = np.linspace(min_lon + 0.25, max_lon - 0.25, num=int(max_lon - min_lon) * 2)  # 0.5° grid\n",
    "    centers_lat = np.linspace(min_lat + 0.25, max_lat - 0.25, num=int(max_lat - min_lat) * 2)\n",
    "    \n",
    "    # Calculate the center of latitude and longitude for each element in a 1° x 1° grid (for comparison purposes)\n",
    "    centers_lon2 = np.linspace(min_lon + 0.5, max_lon - 0.5, num=int(max_lon - min_lon))  # 1° grid\n",
    "    centers_lat2 = np.linspace(min_lat + 0.5, max_lat - 0.5, num=int(max_lat - min_lat))\n",
    "\n",
    "    aux_list = []\n",
    "    \n",
    "    # Calculate the matching FEER coefficient for each element of the 1° x 1° grid\n",
    "    for i in range(0, len(centers_lat2)):\n",
    "        # Extract the FEER coefficients for the current latitude and longitude bounds from the 'dados_feer' DataFrame\n",
    "        df2 = dados_feer.loc[(dados_feer['Latitude'] == centers_lat2[i]) & \n",
    "                             (dados_feer['Longitude'] <= max_lon) &\n",
    "                             (dados_feer['Longitude'] >= min_lon), 'Ce_850'].to_numpy()\n",
    "        \n",
    "        # Append the corresponding FEER coefficients to the list\n",
    "        aux_list = np.append(aux_list, df2)\n",
    "        \n",
    "        if i == 0:\n",
    "            # Create a coordinate pair matrix for the 1° x 1° grid (latitudes and longitudes)\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            lat_lon_feer2 = np.column_stack((aux2, centers_lon2))\n",
    "        else:\n",
    "            aux2 = np.repeat(centers_lat2[i], len(centers_lon2))\n",
    "            aux2 = np.column_stack((aux2, centers_lon2))\n",
    "            lat_lon_feer2 = np.vstack((lat_lon_feer2, aux2))\n",
    "    \n",
    "    # Stack the matching FEER coefficients into the coordinate matrix for the 1° x 1° grid\n",
    "    lat_lon_feer2 = np.column_stack((lat_lon_feer2, aux_list))\n",
    "\n",
    "    # Calculate the FEER coefficient corresponding to each element of the 0.5° x 0.5° grid\n",
    "    for i in range(0, len(centers_lat)):\n",
    "        if i == 0:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            lat_lon_feer = np.column_stack((aux, centers_lon))\n",
    "        else:\n",
    "            aux = np.repeat(centers_lat[i], len(centers_lon))\n",
    "            aux2 = np.column_stack((aux, centers_lon))\n",
    "            lat_lon_feer = np.vstack((lat_lon_feer, aux2))\n",
    "\n",
    "    # Match the corresponding FEER coefficient to each 0.5° x 0.5° grid element based on proximity\n",
    "    aux_list_2 = []\n",
    "    for j in range(0, len(lat_lon_feer)):\n",
    "        for n in range(0, len(lat_lon_feer2)):\n",
    "            if ((lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] + 0.25) or (lat_lon_feer[j, 0] == lat_lon_feer2[n, 0] - 0.25)):\n",
    "                if ((lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] + 0.25) or (lat_lon_feer[j, 1] == lat_lon_feer2[n, 1] - 0.25)):\n",
    "                    # Append the FEER coefficient to the auxiliary list for the matching grid\n",
    "                    aux_list_2 = np.append(aux_list_2, lat_lon_feer2[n, 2])\n",
    "    \n",
    "    # Stack the 0.5° x 0.5° grid coordinates and their corresponding FEER coefficients\n",
    "    lat_lon_feer = np.column_stack((lat_lon_feer, aux_list_2))\n",
    "    matrix = lat_lon_feer  # Final matrix of latitudes, longitudes, and FEER coefficients\n",
    "\n",
    "    # Create a mask for valid values within the entire ROI (used to select corresponding data from Full disk matrix)\n",
    "    I = np.where((rlat >= min_lat) & (rlat <= max_lat) & (rlon >= min_lon) & (rlon <= max_lon))\n",
    "    index_list = []\n",
    "    index_list.insert(0, I)\n",
    "\n",
    "    # Repeat the process for each element of the 0.5° x 0.5° grid to find corresponding indices in the satellite data\n",
    "    for k in range(0, len(matrix)):\n",
    "        aux1 = np.where((rlat >= matrix[k, 0] - 0.25) & (rlat <= matrix[k, 0] + 0.25) & \n",
    "                        (rlon >= matrix[k, 1] - 0.25) & (rlon <= matrix[k, 1] + 0.25))\n",
    "        index_list.insert(k + 1, aux1)\n",
    "\n",
    "    return index_list, matrix  # Return the list of indices and the final grid matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v225DN_TVnaT"
   },
   "outputs": [],
   "source": [
    "# Function to collect and save file names in a list for the given period of interest with error handling\n",
    "# This function iterates over the specified year, day, and hour ranges and collects the corresponding file names\n",
    "# from the NOAA GOES-16 directory structure, with added error handling for missing files.\n",
    "\n",
    "def get_files(s_year, e_year, s_day, e_day, s_hour, e_hour):\n",
    "    print('Getting file names')\n",
    "    aux = []  # List to store the file names\n",
    "    # Loop over the years in the specified range\n",
    "    for y in range(s_year, e_year + 1):\n",
    "        # Loop over the days in the specified range\n",
    "        for d in range(s_day, e_day):\n",
    "            # The variable 'd' determines the days of the product (e.g., day 228 corresponds to 15:00, 15:10, etc.)\n",
    "            for j in range(s_hour, e_hour):\n",
    "                try:\n",
    "                    # List the files for a specific year, day, and hour directory\n",
    "                    # These directories contain 6 files for each 10-minute interval (e.g., 15:00, 15:10, ..., 15:50 UTC)\n",
    "                    FD = fs.ls('noaa-goes16/ABI-L2-FDCF/' + str(y) + '/' + str(d).zfill(3) + '/' + str(j).zfill(2) + '/')\n",
    "                    aux = np.append(aux, FD)  # Append the found files to the list\n",
    "                except FileNotFoundError as e:\n",
    "                    # In case a file is not found, print an error message and skip to the next file\n",
    "                    print(f\"FileNotFoundError file {'noaa-goes16/ABI-L2-FDCF/'+str(y)+'/'+str(d).zfill(3)+'/'+str(j).zfill(2)+'/'}: {e}. Skipping this file.\")\n",
    "                    continue  # Skip to the next file in the list\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ4Wr1OdxYOx"
   },
   "outputs": [],
   "source": [
    "# Main function that processes the input files and computes emission flux estimates for the region of interest (ROI)\n",
    "# The function calculates the emission flux for FRP and burned area using the FEER coefficients and stores the results\n",
    "# in an array, to be later used for spatial and temporal analysis.\n",
    "def process_data_v7(rlat, rlon, files, matrix, indexes):\n",
    "\n",
    "    # Loop through the list of files to process each one\n",
    "    for i in range(0, len(files)):\n",
    "        with fs.open(files[i], 'rb') as f:\n",
    "            # Open the current file using xarray and read the data\n",
    "            ds = xr.open_dataset(BytesIO(f.read()), engine='h5netcdf')\n",
    "            try:\n",
    "                # Extract date and time information from the file path for later use in plots or metadata\n",
    "                prodbase = files[i].split('/')[5][:23]\n",
    "                starttime = files[i].split(prodbase)[1].split('_')[0]\n",
    "                year, julian, hhmm = starttime[:4], starttime[4:7], starttime[7:11]\n",
    "                plottitle = year + ',' + julian + ',' + hhmm\n",
    "                fpart = starttime + ',' + plottitle\n",
    "                print(f'Processing year: {year}, day: {julian}, hour: {hhmm}')  # Processing status message\n",
    "\n",
    "                # Convert hours and minutes into a percentage of the day for easier plotting\n",
    "                code = int(julian) + (int(hhmm) / 100) / 24 + (int(hhmm) % 100) / 60 / 24\n",
    "\n",
    "                #####################################################################\n",
    "                # Extract the FRP (Fire Radiative Power) and Burned Area data from the file\n",
    "                P = np.array(ds.Power)  # FRP Matrix data\n",
    "                A = np.array(ds.Area)   # Burned Area Matrix data\n",
    "\n",
    "                # Use the indexes matrix to collect data from the region of interest (ROI) based on the grid\n",
    "                P_box_amazon = P[indexes[1]]\n",
    "                A_box_amazon = A[indexes[1]]\n",
    "\n",
    "                # Calculate the emission flux as FRP divided by the burned area (P / A)\n",
    "                P_A_box_amazon = P_box_amazon / A_box_amazon\n",
    "\n",
    "                # Remove any NaN values from the resulting array to reduce processing time\n",
    "                array_F_box_amazon = P_A_box_amazon[~np.isnan(P_A_box_amazon)]\n",
    "\n",
    "                # Get the latitude and longitude for the current grid element\n",
    "                lat = matrix[0, 0]  # Latitude of the grid element\n",
    "                lon = matrix[0, 1]  # Longitude of the grid element\n",
    "\n",
    "                # Handle potential NaN values by ignoring warnings and computing the mean emission flux and other species' fluxes\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                    mean_flux = np.mean(array_F_box_amazon)  # Mean flux of FRP\n",
    "                    RE_flux = np.mean(array_F_box_amazon * matrix[0, 2])  # Mean flux of TPM (total particulate matter)\n",
    "                    CO2_flux = np.mean(array_F_box_amazon * Ce_CO2 * matrix[0, 2])  # Mean CO2 flux\n",
    "                    CO_flux = np.mean(array_F_box_amazon * Ce_CO * matrix[0, 2])  # Mean CO flux\n",
    "                    CH4_flux = np.mean(array_F_box_amazon * Ce_CH4 * matrix[0, 2])  # Mean CH4 flux\n",
    "\n",
    "                # If the computed fluxes are NaN, set them to 0 to avoid invalid results\n",
    "                mean_flux = 0 if np.isnan(mean_flux) else mean_flux\n",
    "                RE_flux = 0 if np.isnan(RE_flux) else RE_flux\n",
    "                CO2_flux = 0 if np.isnan(CO2_flux) else CO2_flux\n",
    "                CO_flux = 0 if np.isnan(CO_flux) else CO_flux\n",
    "                CH4_flux = 0 if np.isnan(CH4_flux) else CH4_flux\n",
    "\n",
    "                # Store the computed values for the grid element\n",
    "                array_lat = lat\n",
    "                array_lon = lon\n",
    "                array_mean_flux = mean_flux\n",
    "                array_mean_TPM = RE_flux\n",
    "                array_CO2_flux = CO2_flux\n",
    "                array_CO_flux = CO_flux\n",
    "                array_CH4_flux = CH4_flux\n",
    "\n",
    "                # Repeat the process for each grid element in the matrix\n",
    "                for k in range(1, len(matrix)):\n",
    "                    P_box_amazon = P[indexes[k + 1]]\n",
    "                    A_box_amazon = A[indexes[k + 1]]\n",
    "                    P_A_box_amazon = P_box_amazon / A_box_amazon\n",
    "                    array_F_box_amazon = P_A_box_amazon[~np.isnan(P_A_box_amazon)]\n",
    "\n",
    "                    lat = matrix[k, 0]  # Latitude of the grid element\n",
    "                    lon = matrix[k, 1]  # Longitude of the grid element\n",
    "\n",
    "                    # Calculate mean fluxes for each species and handle NaN values\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                        mean_flux = np.mean(array_F_box_amazon)\n",
    "                        RE_flux = np.mean(array_F_box_amazon * matrix[k, 2])\n",
    "                        CO2_flux = np.mean(array_F_box_amazon * Ce_CO2 * matrix[k, 2])\n",
    "                        CO_flux = np.mean(array_F_box_amazon * Ce_CO * matrix[k, 2])\n",
    "                        CH4_flux = np.mean(array_F_box_amazon * Ce_CH4 * matrix[k, 2])\n",
    "\n",
    "                    # Replace NaN values with 0\n",
    "                    mean_flux = 0 if np.isnan(mean_flux) else mean_flux\n",
    "                    RE_flux = 0 if np.isnan(RE_flux) else RE_flux\n",
    "                    CO2_flux = 0 if np.isnan(CO2_flux) else CO2_flux\n",
    "                    CO_flux = 0 if np.isnan(CO_flux) else CO_flux\n",
    "                    CH4_flux = 0 if np.isnan(CH4_flux) else CH4_flux\n",
    "\n",
    "                    # Append the results for each grid element to the arrays\n",
    "                    array_lat = np.append(array_lat, lat)\n",
    "                    array_lon = np.append(array_lon, lon)\n",
    "                    array_mean_flux = np.append(array_mean_flux, mean_flux)\n",
    "                    array_mean_TPM = np.append(array_mean_TPM, RE_flux)\n",
    "                    array_CO2_flux = np.append(array_CO2_flux, CO2_flux)\n",
    "                    array_CO_flux = np.append(array_CO_flux, CO_flux)\n",
    "                    array_CH4_flux = np.append(array_CH4_flux, CH4_flux)\n",
    "\n",
    "            # Handle any errors that may occur during processing\n",
    "            except OSError as error:\n",
    "                print(error)\n",
    "\n",
    "    # Return the results as arrays containing latitude, longitude, and the corresponding fluxes for each species\n",
    "    return array_lat, array_lon, array_mean_flux, array_mean_TPM, array_CO2_flux, array_CO_flux, array_CH4_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6sZuoCvuCkr",
    "outputId": "af826d68-6a7f-4aeb-9f06-71b91c45fbfd"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Starting message\n",
    "print('Compiling statistics')\n",
    "\n",
    "# Retrieve latitude and longitude data using the get_lat_lon function\n",
    "rlat, rlon = get_lat_lon(fs)\n",
    "\n",
    "# Get the matrix and indexes for the geographic area defined by minlon, maxlon, minlat, maxlat\n",
    "Indexes, M = get_indexes_v3(minlon, maxlon, minlat, maxlat, rlat, rlon, feer_data)\n",
    "print('Got indexes and matrix')\n",
    "\n",
    "# List of month names from the calendar library (excluding the first element, which is empty)\n",
    "month_names = list(calendar.month_name)[1:]\n",
    "\n",
    "# Initialize an empty list to store the matrix of days for each month\n",
    "matrix_days_months = []\n",
    "\n",
    "# Initialize julian_day to 1 (starting point for day counting)\n",
    "julian_day = 1\n",
    "\n",
    "# Loop through each month to populate the matrix with month names and corresponding Julian days\n",
    "for mes_num in range(1, 13):\n",
    "    # Get the last day of the current month using the calendar.monthrange function\n",
    "    last_day_month = calendar.monthrange(Year, mes_num)[1]\n",
    "    \n",
    "    # Generate an array of Julian days for the current month\n",
    "    Julian_days = np.arange(julian_day, julian_day + last_day_month)\n",
    "\n",
    "    # Append the month name and Julian days to the matrix\n",
    "    matrix_days_months.append([month_names[mes_num - 1], Julian_days])\n",
    "\n",
    "    # Update julian_day for the next month\n",
    "    julian_day += last_day_month\n",
    "\n",
    "# Convert the matrix of days/months to a numpy array\n",
    "matrix_days_months = np.array(matrix_days_months, dtype=object)\n",
    "\n",
    "# Loop through specific months (in this case, months 7 and 8, corresponding to July and August)\n",
    "for m in range(7, 9):\n",
    "    # Define the start and end year, days, and hours for data collection based on the selected month\n",
    "    start_year, end_year, start_day, end_day, start_hour, end_hour = Year, Year, \\\n",
    "        matrix_days_months[m, 1][0], matrix_days_months[m, 1][1], 15, 16\n",
    "\n",
    "    # List the data files corresponding to the start and end definitions\n",
    "    data_list = get_files(start_year, end_year, start_day, end_day, start_hour, end_hour)\n",
    "    print('Data listed for ' + str(matrix_days_months[m, 0]))\n",
    "\n",
    "    # Change the working directory to the output directory\n",
    "    os.chdir(datadir)\n",
    "\n",
    "    # Call the process_data_v7 function to retrieve emission flux data for the defined period\n",
    "    array_lat, array_lon, array_mean_flux, array_mean_TPM, array_CO2_flux, array_CO_flux, array_CH4_flux = process_data_v7(rlat, rlon, data_list, M, Indexes)\n",
    "\n",
    "    # Get unique latitude and longitude values from the results\n",
    "    lats = np.unique(array_lat)\n",
    "    lons = np.unique(array_lon)\n",
    "    \n",
    "    # Reverse the latitude array to align with the correct geographic orientation\n",
    "    lats = lats[::-1]\n",
    "\n",
    "    # Loop through each grid element (latitude and longitude) to calculate monthly averages\n",
    "    for k in range(0, len(lats)):\n",
    "        for n in range(0, len(lons)):\n",
    "            # Find the index of the current grid element in the data arrays\n",
    "            index = np.where((array_lat == lats[k]) & (array_lon == lons[n]))\n",
    "\n",
    "            # Calculate the monthly average for each emission flux type\n",
    "            mean_flux_montly = np.mean(array_mean_flux[index])\n",
    "            mean_TPM_montly = np.mean(array_mean_TPM[index])\n",
    "            mean_CO2_montly = np.mean(array_CO2_flux[index])\n",
    "            mean_C0_montly = np.mean(array_CO_flux[index])\n",
    "            mean_CH4_montly = np.mean(array_CH4_flux[index])\n",
    "\n",
    "            # Format the results as a string to be written to the output file\n",
    "            outstring = str(Year) + ',' + str(matrix_days_months[m, 0]) + ',' + str(lats[k]) + ',' + str(lons[n]) + ',' \\\n",
    "                        + str(mean_flux_montly) + ',' + str(mean_TPM_montly) + ',' + str(mean_CO2_montly) + ',' \\\n",
    "                        + str(mean_C0_montly) + ',' + str(mean_CH4_montly) + '\\n'\n",
    "            \n",
    "            # Write the formatted string to the output file\n",
    "            outfn.writelines(outstring)\n",
    "\n",
    "# Close the output file after processing\n",
    "outfn.close()\n",
    "\n",
    "# Print the completion message\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
